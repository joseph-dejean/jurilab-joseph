"""
Script pour ing√©rer TOUS les codes d'un coup
Sauvegarde la liste des codes pour √©viter de refaire la recherche r√©cursive
"""

import json
from pathlib import Path
from ingestion.ingest_from_legi_extracted import (
    find_code_directories,
    ingest_code_from_legi,
    CODE_MAPPING,
    LEGI_EXTRACTED_DIR,
)
from ingestion.ingestion_massive import MassiveIngester
from config.logging_config import get_logger
from config.settings import get_settings

logger = get_logger(__name__)

# Fichier de cache pour la liste des codes
CODES_CACHE_FILE = Path("data/cache/codes_list.json")


def get_all_codes(legi_dir: Path, force_refresh: bool = False) -> list[str]:
    """
    R√©cup√®re la liste de tous les codes, avec cache
    
    Args:
        legi_dir: Dossier LEGI
        force_refresh: Forcer la recherche m√™me si le cache existe
    
    Returns:
        Liste des IDs de codes
    """
    # V√©rifier le cache
    if not force_refresh and CODES_CACHE_FILE.exists():
        logger.info("üìÇ Chargement de la liste des codes depuis le cache...")
        try:
            with open(CODES_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                codes = cache_data.get('codes', [])
                if codes:
                    logger.info(f"‚úÖ {len(codes)} codes charg√©s depuis le cache")
                    return codes
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lecture cache: {e}")
    
    # Recherche r√©cursive (prend ~40 secondes)
    logger.info("üîç Recherche de tous les codes (cela peut prendre ~40 secondes)...")
    code_dirs = find_code_directories(legi_dir)
    
    if not code_dirs:
        logger.error("‚ùå Aucun code trouv√©")
        return []
    
    codes = [d.name for d in code_dirs]
    
    # Sauvegarder dans le cache
    CODES_CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(CODES_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump({
            'codes': codes,
            'count': len(codes),
            'legi_dir': str(legi_dir)
        }, f, indent=2, ensure_ascii=False)
    
    logger.info(f"‚úÖ {len(codes)} codes trouv√©s et sauvegard√©s dans le cache")
    return codes


def main():
    """Ing√®re tous les codes"""
    legi_dir = Path(LEGI_EXTRACTED_DIR)
    
    if not legi_dir.exists():
        logger.error(f"‚ùå Dossier non trouv√©: {legi_dir}")
        return 1
    
    logger.info("=" * 70)
    logger.info("üöÄ INGESTION COMPL√àTE DE TOUS LES CODES")
    logger.info("=" * 70)
    logger.info(f"üìÇ Dossier source: {legi_dir}")
    logger.info("")
    
    # R√©cup√©rer tous les codes (avec cache)
    codes_to_ingest = get_all_codes(legi_dir)
    
    if not codes_to_ingest:
        logger.error("‚ùå Aucun code √† ing√©rer")
        return 1
    
    logger.info(f"üìã {len(codes_to_ingest)} codes √† ing√©rer")
    logger.info("")
    
    # Ing√©rer chaque code
    ingester = MassiveIngester(max_articles=None)  # Tous les articles
    total_articles = 0
    codes_success = 0
    codes_failed = 0
    
    for idx, code_id in enumerate(codes_to_ingest, 1):
        code_name = CODE_MAPPING.get(code_id, code_id)
        logger.info("=" * 70)
        logger.info(f"üìö [{idx}/{len(codes_to_ingest)}] {code_name} ({code_id})")
        logger.info("=" * 70)
        
        try:
            articles = ingest_code_from_legi(
                code_id=code_id,
                legi_dir=legi_dir,
                max_articles=None  # Tous les articles
            )
            
            if articles:
                # Exporter
                code_info = {
                    "id": code_id,
                    "name": code_name,
                }
                export_path = ingester._export_articles(
                    code_name=code_name.lower().replace(" ", "_"),
                    code_info=code_info,
                    articles=articles
                )
                total_articles += len(articles)
                codes_success += 1
                size_mb = export_path.stat().st_size / (1024 * 1024)
                logger.success(f"   ‚úÖ {len(articles)} articles export√©s ({size_mb:.2f} MB)")
            else:
                logger.warning(f"   ‚ö†Ô∏è Aucun article pour {code_id}")
                codes_failed += 1
        
        except Exception as e:
            codes_failed += 1
            logger.error(f"   ‚ùå Erreur pour {code_id}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue
        
        logger.info("")
    
    # R√©sum√©
    logger.info("")
    logger.info("=" * 70)
    logger.info("üìä R√âSUM√â DE L'INGESTION")
    logger.info("=" * 70)
    logger.success(f"‚úÖ Codes r√©ussis: {codes_success}/{len(codes_to_ingest)}")
    if codes_failed > 0:
        logger.warning(f"‚ö†Ô∏è Codes √©chou√©s: {codes_failed}/{len(codes_to_ingest)}")
    logger.success(f"‚úÖ Total articles ing√©r√©s: {total_articles:,}")
    logger.info("=" * 70)
    
    # Lister les fichiers export√©s
    if total_articles > 0:
        logger.info("")
        logger.info("üìÅ Fichiers JSONL g√©n√©r√©s:")
        settings = get_settings()
        export_files = list(settings.EXPORT_DIR.glob("*.jsonl"))
        export_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        total_size_mb = sum(f.stat().st_size for f in export_files) / (1024 * 1024)
        logger.info(f"   Total: {len(export_files)} fichiers, {total_size_mb:.2f} MB")
        logger.info("")
        logger.info("üí° Prochaine √©tape: Uploader vers GCS")
        logger.info("   gsutil -m cp data\\exports\\*.jsonl gs://legal-rag-data-sofia-2025/")
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())

