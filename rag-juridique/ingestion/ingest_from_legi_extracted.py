"""
Script pour ing√©rer les codes depuis l'archive Freemium extraite dans C:/LEGI

Usage:
    python ingestion/ingest_from_legi_extracted.py --code civil
    python ingestion/ingest_from_legi_extracted.py --all
"""

import argparse
from pathlib import Path
from typing import Dict, List, Optional, Any

from config.logging_config import get_logger
from ingestion.ingestion_massive import MassiveIngester
from ingestion.sources.dila_opendata import DILAOpendataClient

logger = get_logger(__name__)

# Chemin de l'archive extraite
# Structure: C:/LEGI/legi/global/code_et_TNC_en_vigueur/code_en_vigueur/LEGI/LEGITEXT*/
LEGI_EXTRACTED_DIR = Path("C:/LEGI/legi/global/code_et_TNC_en_vigueur/code_en_vigueur")

# Mapping code_id ‚Üí nom
CODE_MAPPING = {
    "LEGITEXT000006070721": "Code civil",
    "LEGITEXT000006070716": "Code p√©nal",
    "LEGITEXT000006072050": "Code du travail",
    "LEGITEXT000005634379": "Code de commerce",
    "LEGITEXT000006071164": "Code de proc√©dure civile",
    "LEGITEXT000006071165": "Code de proc√©dure p√©nale",
    "LEGITEXT000006073189": "Code de la s√©curit√© sociale",
}

# Cache global pour √©viter de refaire la recherche √† chaque fois
_CODE_DIRS_CACHE: Dict[Path, List[Path]] = {}


def find_code_directories(legi_dir: Path) -> List[Path]:
    """
    Trouve les dossiers de codes dans l'archive extraite
    
    Args:
        legi_dir: Dossier legi/global/code_et_TNC_en_vigueur
    
    Returns:
        Liste des dossiers LEGITEXT*
    """
    if not legi_dir.exists():
        logger.error(f"‚ùå Dossier non trouv√©: {legi_dir}")
        return []
    
    logger.info(f"üîç Recherche des codes dans: {legi_dir}")
    
    # Structure Freemium : LEGI/TEXT/00/00/06/07/07/LEGITEXT*/
    # Chercher sp√©cifiquement dans LEGI/TEXT/
    legi_text_dir = legi_dir / "LEGI" / "TEXT"
    if legi_text_dir.exists():
        logger.info(f"   üìÇ Cherchant dans: {legi_text_dir}")
        try:
            # Recherche r√©cursive dans TEXT/ pour trouver tous les LEGITEXT*
            code_dirs = []
            for path in legi_text_dir.rglob("LEGITEXT*"):
                if path.is_dir() and path.name.startswith("LEGITEXT"):
                    code_dirs.append(path)
            
            if code_dirs:
                # D√©dupliquer (garder les plus profonds = les vrais dossiers de codes)
                from collections import defaultdict
                by_name = defaultdict(list)
                for d in code_dirs:
                    by_name[d.name].append(d)
                
                # Garder le chemin le plus long (le plus profond)
                code_dirs = [max(dirs, key=lambda x: len(str(x))) for dirs in by_name.values()]
                logger.info(f"   ‚úÖ {len(code_dirs)} codes trouv√©s dans LEGI/TEXT/")
                return code_dirs
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è Erreur lecture LEGI/TEXT/: {e}")
    
    # Structure possible 1: code_en_vigueur/LEGI/LEGITEXT*/ (directement dans LEGI)
    legi_subdir = legi_dir / "LEGI"
    if legi_subdir.exists():
        logger.info(f"   üìÇ Cherchant dans: {legi_subdir}")
        try:
            code_dirs = [d for d in legi_subdir.iterdir() if d.is_dir() and d.name.startswith("LEGITEXT")]
            if code_dirs:
                logger.info(f"   ‚úÖ {len(code_dirs)} codes trouv√©s dans LEGI/")
                return code_dirs
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è Erreur lecture LEGI/: {e}")
    
    # Structure possible 2: code_en_vigueur/LEGITEXT*/ (directement)
    logger.info(f"   üìÇ Cherchant directement dans: {legi_dir}")
    try:
        code_dirs = [d for d in legi_dir.iterdir() if d.is_dir() and d.name.startswith("LEGITEXT")]
        if code_dirs:
            logger.info(f"   ‚úÖ {len(code_dirs)} codes trouv√©s directement")
            return code_dirs
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Erreur lecture directe: {e}")
    
    # Structure possible 3: Recherche r√©cursive g√©n√©rale (limite de profondeur)
    logger.info(f"   üìÇ Recherche r√©cursive g√©n√©rale (profondeur limit√©e)...")
    try:
        code_dirs = []
        # Limiter √† 10 niveaux de profondeur pour √©viter de scanner tout C:\LEGI
        for path in legi_dir.rglob("LEGITEXT*"):
            if path.is_dir() and path.name.startswith("LEGITEXT"):
                # Limiter la profondeur (max 10 niveaux depuis legi_dir)
                depth = len(path.relative_to(legi_dir).parts)
                if depth <= 10:
                    code_dirs.append(path)
        
        if code_dirs:
            # D√©dupliquer (garder les plus profonds)
            from collections import defaultdict
            by_name = defaultdict(list)
            for d in code_dirs:
                by_name[d.name].append(d)
            
            code_dirs = [max(dirs, key=lambda x: len(str(x))) for dirs in by_name.values()]
            logger.info(f"   ‚úÖ {len(code_dirs)} codes trouv√©s (recherche g√©n√©rale)")
            return code_dirs
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Erreur recherche r√©cursive: {e}")
    
    logger.warning(f"   ‚ö†Ô∏è Aucun code LEGITEXT* trouv√©")
    try:
        dirs_present = [d.name for d in legi_dir.iterdir() if d.is_dir()][:10]
        logger.info(f"   üìÇ Dossiers pr√©sents: {dirs_present}")
    except:
        pass
    return []


def ingest_code_from_legi(
    code_id: str,
    legi_dir: Path,
    max_articles: Optional[int] = None
) -> List[Dict[str, Any]]:
    """
    Ing√®re un code depuis C:/LEGI
    
    Args:
        code_id: ID du code (ex: LEGITEXT000006070721)
        legi_dir: Dossier legi/global/code_et_TNC_en_vigueur
        max_articles: Nombre maximum d'articles (None = tous)
    
    Returns:
        Liste d'articles au format Vertex AI
    """
    logger.info(f"üìö Ingestion: {CODE_MAPPING.get(code_id, code_id)}")
    
    # Essayer d'abord un chemin direct (plus rapide)
    # Structure: LEGI/TEXT/00/00/06/07/07/LEGITEXT000006070721
    code_dir = None
    
    # Extraire les chiffres de l'ID (ex: LEGITEXT000006070721 -> 000006070721)
    if code_id.startswith("LEGITEXT"):
        code_num = code_id.replace("LEGITEXT", "")
        # Construire le chemin: TEXT/00/00/06/07/07/LEGITEXT000006070721
        # Les 10 premiers chiffres d√©terminent le chemin (5 niveaux de 2 chiffres)
        if len(code_num) >= 10:
            path_parts = [code_num[i:i+2] for i in range(0, 10, 2)]  # ['00', '00', '06', '07', '07']
            direct_path = legi_dir / "LEGI" / "TEXT"
            for part in path_parts:
                direct_path = direct_path / part
            direct_path = direct_path / code_id
            if direct_path.exists():
                code_dir = direct_path
                logger.info(f"   ‚úÖ Chemin direct trouv√©: {code_dir}")
    
    # Si le chemin direct ne fonctionne pas, utiliser la recherche r√©cursive (avec cache)
    if not code_dir:
        # Utiliser le cache pour √©viter de refaire la recherche
        global _CODE_DIRS_CACHE
        if legi_dir not in _CODE_DIRS_CACHE:
            logger.info("   üîç Recherche des codes (premi√®re fois, mise en cache)...")
            _CODE_DIRS_CACHE[legi_dir] = find_code_directories(legi_dir)
        else:
            logger.info("   ‚úÖ Utilisation du cache des codes")
        
        code_dirs = _CODE_DIRS_CACHE[legi_dir]
        
        for d in code_dirs:
            if d.name == code_id:
                code_dir = d
                break
    
    if not code_dir:
        logger.warning(f"   ‚ö†Ô∏è Code {code_id} non trouv√© dans {legi_dir}")
        return []
    
    logger.info(f"   üìÇ Dossier: {code_dir}")
    
    # Utiliser le parser DILA
    client = DILAOpendataClient()
    
    # Trouver les fichiers XML
    xml_files = client.find_xml_files(code_dir)
    
    if not xml_files:
        logger.warning(f"   ‚ö†Ô∏è Aucun fichier XML trouv√©")
        return []
    
    logger.info(f"   üìÑ {len(xml_files)} fichiers XML trouv√©s")
    
    # Parser les fichiers
    all_articles = []
    
    from tqdm import tqdm
    for xml_file in tqdm(xml_files, desc=f"   Parsing {code_id}"):
        articles = client.parse_legi_xml(xml_file)
        if articles:
            all_articles.extend(articles)
        
        if max_articles and len(all_articles) >= max_articles:
            all_articles = all_articles[:max_articles]
            break
    
    logger.success(f"   ‚úÖ {len(all_articles)} articles pars√©s")
    
    # Convertir au format Vertex AI
    code_info = {
        "id": code_id,
        "name": CODE_MAPPING.get(code_id, code_id),
    }
    
    ingester = MassiveIngester(max_articles=max_articles)
    
    articles_vertex = []
    for art in all_articles:
        article = ingester._create_article(
            code_info=code_info,
            article_id=art.get("id", f"{code_id}_{art.get('num', 'UNKNOWN')}"),
            num=art.get("num", ""),
            content=art.get("content", ""),
            breadcrumb=art.get("breadcrumb", ""),
            date_debut=art.get("date_debut", "1804-02-07"),
            date_fin=art.get("date_fin"),
            etat=art.get("etat", "VIGUEUR"),
            source="Freemium LEGI",
        )
        articles_vertex.append(article)
    
    return articles_vertex


def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(description="Ing√©rer les codes depuis C:\\LEGI")
    parser.add_argument("--code", type=str, help="Code √† ing√©rer (ex: civil, penal)")
    parser.add_argument("--all", action="store_true", help="Ing√©rer tous les codes")
    parser.add_argument("--max-articles", type=int, help="Nombre maximum d'articles par code")
    parser.add_argument("--legi-dir", type=str, default=str(LEGI_EXTRACTED_DIR), help="Chemin vers code_en_vigueur (ex: C:/LEGI/legi/global/code_et_TNC_en_vigueur/code_en_vigueur)")
    
    args = parser.parse_args()
    
    legi_dir = Path(args.legi_dir)
    
    if not legi_dir.exists():
        logger.error(f"‚ùå Dossier non trouv√©: {legi_dir}")
        logger.info(f"   V√©rifiez que l'extraction est termin√©e dans C:\\LEGI")
        return 1
    
    logger.info("=" * 70)
    logger.info("üöÄ INGESTION DEPUIS C:\\LEGI")
    logger.info("=" * 70)
    logger.info(f"üìÇ Dossier source: {legi_dir}")
    logger.info("")
    
    # D√©terminer quels codes ing√©rer (sans faire de recherche r√©cursive si un code sp√©cifique est demand√©)
    if args.code:
        # Chercher le code correspondant
        code_name_lower = args.code.lower()
        code_id = None
        
        for code_id_key, code_name in CODE_MAPPING.items():
            if code_name_lower in code_name.lower():
                code_id = code_id_key
                break
        
        if not code_id:
            logger.error(f"‚ùå Code '{args.code}' non trouv√©")
            logger.info(f"   Codes disponibles: {', '.join(CODE_MAPPING.values())}")
            return 1
        
        codes_to_ingest = [code_id]
        logger.info(f"üìã Ingestion: {CODE_MAPPING[code_id]}")
    elif args.all:
        # Pour --all, on doit trouver tous les codes (recherche r√©cursive n√©cessaire)
        logger.info("   üîç Recherche de tous les codes...")
        code_dirs = find_code_directories(legi_dir)
        
        if not code_dirs:
            logger.error("‚ùå Aucun code trouv√© dans l'archive extraite")
            return 1
        
        codes_to_ingest = [d.name for d in code_dirs]
        logger.info(f"üìã Ingestion de TOUS les codes ({len(codes_to_ingest)})")
    else:
        logger.error("‚ùå Sp√©cifiez --code ou --all")
        return 1
    
    logger.info("")
    
    # Ing√©rer chaque code
    ingester = MassiveIngester(max_articles=args.max_articles)
    total_articles = 0
    codes_success = 0
    codes_failed = 0
    
    logger.info(f"üìã Ingestion de {len(codes_to_ingest)} codes...")
    logger.info("")
    
    for idx, code_id in enumerate(codes_to_ingest, 1):
        code_name = CODE_MAPPING.get(code_id, code_id)
        logger.info("=" * 70)
        logger.info(f"üìö [{idx}/{len(codes_to_ingest)}] {code_name} ({code_id})")
        logger.info("=" * 70)
        
        try:
            articles = ingest_code_from_legi(
                code_id=code_id,
                legi_dir=legi_dir,
                max_articles=args.max_articles
            )
            
            if articles:
                # Exporter
                code_info = {
                    "id": code_id,
                    "name": code_name,
                }
                export_path = ingester._export_articles(
                    code_name=code_name.lower().replace(" ", "_"),
                    code_info=code_info,
                    articles=articles
                )
                total_articles += len(articles)
                codes_success += 1
                logger.success(f"   ‚úÖ {len(articles)} articles export√©s: {export_path.name}")
            else:
                logger.warning(f"   ‚ö†Ô∏è Aucun article pour {code_id}")
                codes_failed += 1
        
        except Exception as e:
            codes_failed += 1
            logger.error(f"   ‚ùå Erreur pour {code_id}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue
        
        logger.info("")
    
    logger.info("")
    logger.info("=" * 70)
    logger.info("üìä R√âSUM√â DE L'INGESTION")
    logger.info("=" * 70)
    logger.success(f"‚úÖ Codes r√©ussis: {codes_success}/{len(codes_to_ingest)}")
    if codes_failed > 0:
        logger.warning(f"‚ö†Ô∏è Codes √©chou√©s: {codes_failed}/{len(codes_to_ingest)}")
    logger.success(f"‚úÖ Total articles ing√©r√©s: {total_articles}")
    logger.info("=" * 70)
    
    # Lister les fichiers export√©s
    if total_articles > 0:
        logger.info("")
        logger.info("üìÅ Fichiers JSONL g√©n√©r√©s:")
        from config.settings import get_settings
        settings = get_settings()
        export_files = list(settings.EXPORT_DIR.glob("*.jsonl"))
        export_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        for f in export_files[:10]:  # Afficher les 10 derniers
            size_kb = f.stat().st_size / 1024
            logger.info(f"   - {f.name} ({size_kb:.1f} KB)")
        if len(export_files) > 10:
            logger.info(f"   ... et {len(export_files) - 10} autres fichiers")
    
    logger.info("")
    logger.info("üí° Prochaine √©tape: Uploader les fichiers JSONL vers GCS")
    logger.info("   gsutil -m cp data\\exports\\*.jsonl gs://legal-rag-data-sofia-2025/")
    
    return 0


if __name__ == "__main__":
    exit(main())

