"""
Pilier 5 : Chatbot Avocat
Hub central conversationnel pour l'assistance juridique
"""

import uuid
from datetime import datetime
from typing import Optional

import google.generativeai as genai

from config.logging_config import get_logger
from config.settings import get_settings
from rag.vertex_search import VertexSearchClient
from api.models import (
    ChatMessage,
    ChatRequest,
    ChatResponse,
    Source,
)

logger = get_logger(__name__)
settings = get_settings()

# Configuration Gemini avec clÃ© API
if settings.GEMINI_API_KEY:
    genai.configure(api_key=settings.GEMINI_API_KEY)
    logger.debug("Gemini configurÃ© avec clÃ© API")
else:
    logger.warning("âš ï¸ GEMINI_API_KEY non dÃ©finie dans .env")


class ConversationManager:
    """Gestionnaire de conversations avec historique"""
    
    def __init__(self):
        """Initialise le gestionnaire de conversations"""
        self.conversations: dict[str, list[ChatMessage]] = {}
        logger.debug("ConversationManager initialisÃ©")
    
    def create_conversation(self) -> str:
        """
        CrÃ©e une nouvelle conversation
        
        Returns:
            ID de la conversation
        """
        conv_id = str(uuid.uuid4())
        self.conversations[conv_id] = []
        logger.debug(f"Nouvelle conversation crÃ©Ã©e: {conv_id}")
        return conv_id
    
    def add_message(self, conv_id: str, role: str, content: str) -> None:
        """
        Ajoute un message Ã  la conversation
        
        Args:
            conv_id: ID de la conversation
            role: RÃ´le (user ou assistant)
            content: Contenu du message
        """
        if conv_id not in self.conversations:
            self.conversations[conv_id] = []
        
        message = ChatMessage(
            role=role,
            content=content,
            timestamp=datetime.now()
        )
        self.conversations[conv_id].append(message)
    
    def get_history(self, conv_id: str, max_messages: int = 10) -> list[ChatMessage]:
        """
        RÃ©cupÃ¨re l'historique d'une conversation
        
        Args:
            conv_id: ID de la conversation
            max_messages: Nombre maximum de messages Ã  retourner
        
        Returns:
            Liste des derniers messages
        """
        if conv_id not in self.conversations:
            return []
        
        return self.conversations[conv_id][-max_messages:]
    
    def clear_conversation(self, conv_id: str) -> None:
        """Efface l'historique d'une conversation"""
        if conv_id in self.conversations:
            del self.conversations[conv_id]
            logger.debug(f"Conversation {conv_id} effacÃ©e")


class ChatbotAvocat:
    """
    Chatbot Avocat - Hub central conversationnel
    
    FonctionnalitÃ©s:
    - RÃ©ponses en langage naturel avec Gemini
    - Grounding avec RAG (citations sources)
    - Routage vers les autres outils
    - Historique de conversation
    - Suggestions d'actions
    """
    
    def __init__(self):
        """Initialise le Chatbot Avocat"""
        self.vertex_client = VertexSearchClient()
        self.conversation_manager = ConversationManager()
        
        # Configuration Gemini avec API directe
        try:
            if not settings.GEMINI_API_KEY:
                logger.warning("âš ï¸ GEMINI_API_KEY non dÃ©finie - mode dÃ©gradÃ© activÃ©")
                self.model = None
            else:
                self.model = genai.GenerativeModel(settings.GEMINI_FLASH_MODEL)
                logger.debug(f"âœ… ModÃ¨le Gemini configurÃ©: {settings.GEMINI_FLASH_MODEL}")
        except Exception as e:
            logger.warning(f"âš ï¸ Impossible de configurer Gemini: {e}")
            self.model = None
        
        logger.info("âœ… ChatbotAvocat initialisÃ©")
    
    def chat(self, request: ChatRequest) -> ChatResponse:
        """
        Traite une requÃªte de chat
        
        Args:
            request: RequÃªte de chat avec message et options
        
        Returns:
            RÃ©ponse du chatbot avec sources et suggestions
        """
        logger.info(f"ğŸ’¬ Question: '{request.message}'")
        
        # 1. GÃ©rer la conversation
        conv_id = request.conversation_id or self.conversation_manager.create_conversation()
        
        # 2. Ajouter le message utilisateur
        self.conversation_manager.add_message(conv_id, "user", request.message)
        
        # 3. RAG : Rechercher des sources si activÃ©
        sources = []
        context = ""
        
        if request.use_rag:
            sources, context = self._retrieve_sources(request.message, request.max_sources)
        
        # 4. Construire le prompt avec contexte
        history = self.conversation_manager.get_history(conv_id, max_messages=5)
        prompt = self._build_prompt(request.message, context, history)
        
        # 5. GÃ©nÃ©rer la rÃ©ponse avec Gemini
        response_text, confidence = self._generate_response(prompt)
        
        # 6. Ajouter la rÃ©ponse Ã  l'historique
        self.conversation_manager.add_message(conv_id, "assistant", response_text)
        
        # 7. GÃ©nÃ©rer des suggestions d'actions
        suggested_actions = self._generate_suggestions(request.message, response_text)
        
        # 8. Construire la rÃ©ponse
        response = ChatResponse(
            response=response_text,
            sources=sources,
            conversation_id=conv_id,
            suggested_actions=suggested_actions,
            confidence=confidence,
        )
        
        logger.success(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e (confiance: {confidence:.0%})")
        
        return response
    
    def _retrieve_sources(
        self,
        query: str,
        max_sources: int
    ) -> tuple[list[Source], str]:
        """
        RÃ©cupÃ¨re des sources via RAG
        
        Args:
            query: Question de l'utilisateur
            max_sources: Nombre maximum de sources
        
        Returns:
            Tuple (liste de sources, contexte formatÃ©)
        """
        try:
            # Recherche dans Vertex AI
            results = self.vertex_client.search(query, page_size=max_sources)
            
            sources = []
            context_parts = []
            
            for i, result in enumerate(results, 1):
                # CrÃ©er l'objet Source
                source = Source(
                    type="code" if "article" in result.get("title", "").lower() else "jurisprudence",
                    reference=result.get("title", ""),
                    text=result.get("content", "")[:300] + "...",  # Tronquer
                    relevance=result.get("score", 0.0) or 0.0,
                )
                sources.append(source)
                
                # Construire le contexte pour le prompt
                metadata = result.get("metadata", {})
                breadcrumb = metadata.get("breadcrumb", "")
                
                context_parts.append(
                    f"[Source {i}] {result.get('title', 'N/A')}\n"
                    f"RÃ©fÃ©rence: {breadcrumb}\n"
                    f"Contenu: {result.get('content', 'N/A')}\n"
                )
            
            context = "\n".join(context_parts)
            
            logger.debug(f"âœ… {len(sources)} source(s) rÃ©cupÃ©rÃ©e(s)")
            
            return sources, context
            
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur rÃ©cupÃ©ration sources: {e}")
            return [], ""
    
    def _build_prompt(
        self,
        question: str,
        context: str,
        history: list[ChatMessage]
    ) -> str:
        """
        Construit le prompt pour Gemini
        
        Args:
            question: Question de l'utilisateur
            context: Contexte RAG
            history: Historique de conversation
        
        Returns:
            Prompt complet
        """
        # Prompt systÃ¨me
        system_prompt = """Tu es un assistant juridique expert spÃ©cialisÃ© en droit franÃ§ais.

RÃ”LE:
- RÃ©ponds de maniÃ¨re claire, prÃ©cise et pÃ©dagogique
- Cite TOUJOURS tes sources (articles de loi, rÃ©fÃ©rences juridiques)
- Si tu n'es pas sÃ»r, dis-le clairement
- Utilise un langage professionnel mais accessible

RÃˆGLES:
1. Base-toi UNIQUEMENT sur les sources fournies (ne pas inventer)
2. Cite les articles avec leur rÃ©fÃ©rence complÃ¨te
3. Structure ta rÃ©ponse (dÃ©finition, rÃ¨gles, exceptions, exemples)
4. Si les sources sont insuffisantes, indique-le
5. Ne donne JAMAIS de conseil juridique personnalisÃ© (tu n'es pas avocat)

FORMAT DE RÃ‰PONSE:
- Introduction courte
- DÃ©veloppement avec citations
- Conclusion synthÃ©tique
- [Sources utilisÃ©es] Ã  la fin
"""
        
        # Historique de conversation
        history_text = ""
        if history:
            history_text = "\n\nHISTORIQUE DE CONVERSATION:\n"
            for msg in history[:-1]:  # Exclure le dernier (question actuelle)
                role_emoji = "ğŸ‘¤" if msg.role == "user" else "ğŸ¤–"
                history_text += f"{role_emoji} {msg.role}: {msg.content}\n"
        
        # Contexte RAG
        context_text = ""
        if context:
            context_text = f"\n\nSOURCES JURIDIQUES DISPONIBLES:\n{context}\n"
        
        # Assemblage final
        prompt = f"""{system_prompt}
{history_text}
{context_text}

QUESTION ACTUELLE:
{question}

RÃ‰PONSE:
"""
        
        return prompt
    
    def _generate_response(self, prompt: str) -> tuple[str, float]:
        """
        GÃ©nÃ¨re une rÃ©ponse avec Gemini
        
        Args:
            prompt: Prompt complet
        
        Returns:
            Tuple (rÃ©ponse, score de confiance)
        """
        if not self.model:
            logger.error("âŒ ModÃ¨le Gemini non initialisÃ©")
            fallback = (
                "Le modÃ¨le d'IA n'est pas disponible. "
                "Veuillez configurer les credentials Google Cloud."
            )
            return fallback, 0.0
        
        try:
            # Configuration de gÃ©nÃ©ration
            generation_config = genai.types.GenerationConfig(
                temperature=0.3,  # Peu crÃ©atif (factuel)
                top_p=0.95,
                top_k=40,
                max_output_tokens=1024,
            )
            
            # GÃ©nÃ©ration avec Gemini (API directe)
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )
            
            response_text = response.text
            
            # Estimation de confiance basique
            # TODO: AmÃ©liorer avec analyse de la rÃ©ponse
            confidence = 0.85  # Par dÃ©faut
            
            if "je ne sais pas" in response_text.lower() or "insuffisant" in response_text.lower():
                confidence = 0.4
            elif "sources" in response_text.lower() and "article" in response_text.lower():
                confidence = 0.95
            
            return response_text, confidence
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration Gemini: {e}")
            
            # RÃ©ponse de secours basÃ©e sur les sources RAG
            # Si on a des sources dans le prompt, on peut au moins les prÃ©senter
            if "SOURCE" in prompt and "SOURCES JURIDIQUES DISPONIBLES" in prompt:
                # Extraire les sources du prompt
                fallback = self._generate_fallback_from_sources(prompt)
                return fallback, 0.6
            else:
                fallback = (
                    "âš ï¸ Le service de gÃ©nÃ©ration de rÃ©ponse est temporairement indisponible. "
                    "Cependant, j'ai trouvÃ© des sources pertinentes ci-dessus."
                )
                return fallback, 0.3
    
    def _generate_fallback_from_sources(self, prompt: str) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse de secours basÃ©e sur les sources RAG
        
        Args:
            prompt: Prompt contenant les sources
        
        Returns:
            RÃ©ponse basique avec rÃ©fÃ©rences aux sources
        """
        # Extraire la section des sources
        if "SOURCES JURIDIQUES DISPONIBLES:" in prompt:
            sources_section = prompt.split("SOURCES JURIDIQUES DISPONIBLES:")[1]
            sources_section = sources_section.split("QUESTION ACTUELLE:")[0].strip()
            
            # Compter les sources
            source_count = sources_section.count("[Source")
            
            response = f"""Voici ce que j'ai trouvÃ© dans les sources juridiques ({source_count} sources) :

{sources_section[:800]}

âš ï¸ Note : Le service de gÃ©nÃ©ration automatique est temporairement indisponible. 
Les sources ci-dessus contiennent les informations pertinentes pour rÃ©pondre Ã  votre question.
Veuillez consulter les articles mentionnÃ©s pour plus de dÃ©tails."""
            
            return response
        
        return "Sources trouvÃ©es mais impossible de les formater."
    
    def _generate_suggestions(self, question: str, response: str) -> list[str]:
        """
        GÃ©nÃ¨re des suggestions d'actions basÃ©es sur la conversation
        
        Args:
            question: Question de l'utilisateur
            response: RÃ©ponse gÃ©nÃ©rÃ©e
        
        Returns:
            Liste de suggestions
        """
        suggestions = []
        
        # Analyse simple basÃ©e sur les mots-clÃ©s
        question_lower = question.lower()
        response_lower = response.lower()
        
        # Si on parle de contrat
        if "contrat" in question_lower or "contrat" in response_lower:
            suggestions.append("ğŸ” Rechercher la jurisprudence sur les contrats")
            suggestions.append("ğŸ“„ GÃ©nÃ©rer un modÃ¨le de contrat")
        
        # Si on parle de procÃ©dure
        if any(word in question_lower for word in ["procÃ©dure", "jugement", "tribunal"]):
            suggestions.append("ğŸ“Š Analyser la stratÃ©gie procÃ©durale")
            suggestions.append("ğŸ” Rechercher des cas similaires")
        
        # Si on parle de conformitÃ©
        if any(word in question_lower for word in ["conformitÃ©", "vÃ©rifier", "valide"]):
            suggestions.append("âœ… Auditer un document pour conformitÃ©")
        
        # Suggestions gÃ©nÃ©rales
        if not suggestions:
            suggestions.append("ğŸ” Approfondir cette recherche")
            suggestions.append("ğŸ’¬ Poser une question connexe")
        
        return suggestions[:3]  # Max 3 suggestions
    
    def clear_conversation(self, conv_id: str) -> None:
        """Efface l'historique d'une conversation"""
        self.conversation_manager.clear_conversation(conv_id)


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def quick_chat(message: str, use_rag: bool = True) -> ChatResponse:
    """
    Fonction rapide pour chatter
    
    Args:
        message: Message de l'utilisateur
        use_rag: Utiliser le RAG pour grounding
    
    Returns:
        RÃ©ponse du chatbot
    
    Exemple:
        >>> response = quick_chat("Qu'est-ce qu'un contrat ?")
        >>> print(response.response)
    """
    chatbot = ChatbotAvocat()
    
    request = ChatRequest(
        message=message,
        use_rag=use_rag,
        max_sources=3,
    )
    
    return chatbot.chat(request)


# ============================================================================
# SCRIPT DE TEST
# ============================================================================

if __name__ == "__main__":
    """Test du Chatbot Avocat"""
    
    logger.info("=" * 70)
    logger.info("ğŸ§ª TEST DU CHATBOT AVOCAT")
    logger.info("=" * 70)
    
    # Test 1: Question simple avec RAG
    logger.info("\nğŸ“ Test 1: Question avec RAG")
    response = quick_chat("Qu'est-ce qu'un contrat selon le Code civil ?")
    
    logger.info(f"\nğŸ’¬ RÃ©ponse:")
    print(response.response)
    
    logger.info(f"\nğŸ“š Sources ({len(response.sources)}):")
    for i, source in enumerate(response.sources, 1):
        logger.info(f"   {i}. {source.reference} (pertinence: {source.relevance:.0%})")
    
    logger.info(f"\nğŸ’¡ Suggestions:")
    for suggestion in response.suggested_actions:
        logger.info(f"   â€¢ {suggestion}")
    
    logger.info(f"\nğŸ“Š Confiance: {response.confidence:.0%}")
    
    # Test 2: Conversation avec historique
    logger.info("\nğŸ“ Test 2: Conversation avec historique")
    chatbot = ChatbotAvocat()
    
    # Premier message
    req1 = ChatRequest(message="Quelle est la majoritÃ© en France ?", use_rag=True)
    resp1 = chatbot.chat(req1)
    logger.info(f"\nğŸ‘¤ Q1: {req1.message}")
    logger.info(f"ğŸ¤– R1: {resp1.response[:100]}...")
    
    # DeuxiÃ¨me message (mÃªme conversation)
    req2 = ChatRequest(
        message="Et pour les contrats ?",
        conversation_id=resp1.conversation_id,
        use_rag=True
    )
    resp2 = chatbot.chat(req2)
    logger.info(f"\nğŸ‘¤ Q2: {req2.message}")
    logger.info(f"ğŸ¤– R2: {resp2.response[:100]}...")
    
    logger.info("\n" + "=" * 70)
    logger.info("âœ… TESTS TERMINÃ‰S")
    logger.info("=" * 70)

