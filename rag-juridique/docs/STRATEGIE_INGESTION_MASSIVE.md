# üìö Strat√©gie d'Ingestion Massive - Donn√©es Juridiques

## üéØ Objectif

Ing√©rer **TOUTES** les donn√©es juridiques fran√ßaises dans un **seul datastore Vertex AI** avec segmentation par **m√©tadonn√©es**.

## ‚úÖ Pourquoi maintenant ?

1. ‚úÖ **Tous les outils fonctionnent** (5 piliers op√©rationnels)
2. ‚úÖ **API stable** (plus d'erreurs 500 critiques)
3. ‚úÖ **Format Vertex AI valid√©** (test avec 10 articles r√©ussi)
4. ‚úÖ **M√©tadonn√©es structur√©es** (pr√™tes pour segmentation)

## üèóÔ∏è Architecture

### Un seul datastore, segmentation par m√©tadonn√©es

```
datastorerag_1766055384992 (global)
‚îú‚îÄ‚îÄ Code Civil (code_id: LEGITEXT000006070721)
‚îú‚îÄ‚îÄ Code P√©nal (code_id: LEGITEXT000006070716)
‚îú‚îÄ‚îÄ Code du Travail (code_id: LEGITEXT000006072050)
‚îú‚îÄ‚îÄ Code de Commerce (code_id: LEGITEXT000005634379)
‚îî‚îÄ‚îÄ ... (autres codes)
```

### M√©tadonn√©es pour segmentation

Chaque document contient des m√©tadonn√©es permettant le filtrage :

```json
{
  "id": "LEGIARTI000006419101",
  "jsonData": "{
    \"content\": \"...\",
    \"title\": \"Article 1101\",
    \"metadata\": {
      \"code_id\": \"LEGITEXT000006070721\",      // ‚Üê Filtrage par code
      \"code_name\": \"Code civil\",                // ‚Üê Affichage
      \"type\": \"article_code\",                  // ‚Üê Type de document
      \"etat\": \"VIGUEUR\",                       // ‚Üê Filtrage par √©tat
      \"date_debut\": \"2016-10-01\",              // ‚Üê Filtrage temporel
      \"article_num\": \"1101\",                   // ‚Üê Recherche pr√©cise
      \"breadcrumb\": \"Code civil > Livre III...\", // ‚Üê Navigation
      \"source\": \"Hugging Face\"                 // ‚Üê Tra√ßabilit√©
    }
  }"
}
```

## üìã Codes √† ing√©rer (par priorit√©)

| Priorit√© | Code | ID | Articles | Statut |
|----------|------|-----|----------|--------|
| 1 | Code civil | LEGITEXT000006070721 | ~8,000 | ‚è≥ √Ä faire |
| 2 | Code p√©nal | LEGITEXT000006070716 | ~5,000 | ‚è≥ √Ä faire |
| 3 | Code du travail | LEGITEXT000006072050 | ~10,000 | ‚è≥ √Ä faire |
| 4 | Code de commerce | LEGITEXT000005634379 | ~3,000 | ‚è≥ √Ä faire |
| 5 | Code de proc√©dure civile | LEGITEXT000006070716 | ~2,000 | ‚è≥ √Ä faire |
| 6 | Code de proc√©dure p√©nale | LEGITEXT000006071164 | ~2,000 | ‚è≥ √Ä faire |
| 7 | Code de la s√©curit√© sociale | LEGITEXT000006073189 | ~5,000 | ‚è≥ √Ä faire |

**Total estim√© : ~35,000 articles**

## üöÄ Utilisation

### 1. Ing√©rer un code sp√©cifique

```bash
# Code Civil complet
python ingestion/ingestion_massive.py --code civil

# Code P√©nal avec limite (test)
python ingestion/ingestion_massive.py --code penal --max-articles 1000
```

### 2. Ing√©rer tous les codes

```bash
# Tous les codes (ordre de priorit√©)
python ingestion/ingestion_massive.py --all

# Avec limite par code (test)
python ingestion/ingestion_massive.py --all --max-articles 500
```

### 3. Reprendre apr√®s interruption

Le script sauvegarde automatiquement des checkpoints :

```bash
# Le script reprendra automatiquement depuis le dernier checkpoint
python ingestion/ingestion_massive.py --code civil
```

## üì• Strat√©gies d'ingestion (avec fallback)

Le script essaie plusieurs sources dans l'ordre :

1. **Hugging Face** (datasets disponibles)
   - `antoinejeannot/code-civil-fr`
   - `antoinejeannot/french-jurisprudence`

2. **data.gouv.fr / DILA** (t√©l√©chargement direct)
   - Archives LEGI
   - Dumps XML

3. **Fichiers locaux** (XML/JSON)
   - Dossier `data/raw/{code_name}/`

4. **G√©n√©ration enrichie** (fallback)
   - Articles essentiels pr√©-d√©finis
   - Pour tester le pipeline

## üì§ Upload vers Vertex AI

### √âtape 1 : Upload vers Cloud Storage

```bash
# Upload tous les fichiers JSONL
gsutil -m cp data/exports/*.jsonl gs://legal-rag-data-sofia-2025/
```

### √âtape 2 : Import dans Vertex AI Search

1. Aller dans **GCP Console** > **Vertex AI Search**
2. S√©lectionner le datastore : `datastorerag_1766055384992`
3. Cliquer sur **Importer**
4. S√©lectionner les fichiers depuis GCS : `gs://legal-rag-data-sofia-2025/*.jsonl`
5. Lancer l'import

### √âtape 3 : V√©rification

Tester la segmentation par m√©tadonn√©es :

```python
from rag.vertex_search import VertexSearchClient

client = VertexSearchClient()

# Recherche dans Code Civil uniquement
results = client.filter_by_metadata(
    query="contrat",
    code_id="LEGITEXT000006070721",
    etat="VIGUEUR"
)

# Recherche dans tous les codes
results_all = client.search("contrat", page_size=10)
```

## üîç Segmentation par m√©tadonn√©es

### Filtres disponibles

- **Par code** : `code_id="LEGITEXT000006070721"`
- **Par √©tat** : `etat="VIGUEUR"` (VIGUEUR, ABROGE, MODIFIE)
- **Par date** : `date_debut>="2020-01-01"`
- **Par type** : `type="article_code"` (article_code, jurisprudence, etc.)

### Exemples d'utilisation

```python
# Articles en vigueur du Code Civil
client.filter_by_metadata(
    query="contrat",
    code_id="LEGITEXT000006070721",
    etat="VIGUEUR"
)

# Articles modifi√©s apr√®s 2020
client.filter_by_metadata(
    query="travail",
    code_id="LEGITEXT000006072050",
    date_debut_min="2020-01-01"
)
```

## ‚ö†Ô∏è Limitations connues

### Filtres Vertex AI

Les filtres sur champs nested (`metadata.etat`) peuvent ne pas fonctionner selon la configuration Vertex AI. Dans ce cas :

- **Solution 1** : Utiliser la recherche s√©mantique pure (fonctionne toujours)
- **Solution 2** : Filtrer c√¥t√© application apr√®s r√©cup√©ration
- **Solution 3** : Restructurer le JSONL (m√©tadonn√©es au niveau racine)

### Volume de donn√©es

- **Limite Vertex AI** : ~1M documents par datastore
- **Notre volume** : ~35,000 articles (bien en dessous)
- **Marge** : Place pour jurisprudence et autres sources

## üìä Monitoring

### Checkpoints

Le script sauvegarde automatiquement :
- `data/checkpoints/{code_name}_checkpoint.json` : Progression par code
- `data/checkpoints/global_checkpoint.json` : Statistiques globales

### Logs

Tous les logs sont dans :
- Console (temps r√©el)
- `logs/legal_rag.log` (fichier)

### Statistiques

√Ä la fin de l'ingestion :
```
‚úÖ INGESTION MASSIVE TERMIN√âE
üìä Statistiques:
   - Codes trait√©s: 7
   - Articles totaux: 35,000
   - Erreurs: 0
   - Dur√©e: 1234.5 secondes
```

## üéØ Prochaines √©tapes apr√®s ingestion

1. **Tester les 5 piliers** avec donn√©es compl√®tes
2. **Valider la segmentation** (filtres par code/√©tat)
3. **Ajouter jurisprudence** (Hugging Face datasets)
4. **Optimiser les prompts** (plus de contexte disponible)
5. **Am√©liorer les filtres** (si n√©cessaire)

## üí° Conseils

- **Commencer petit** : Tester avec `--max-articles 100` d'abord
- **Un code √† la fois** : Valider le pipeline avant d'ing√©rer tous les codes
- **V√©rifier les m√©tadonn√©es** : S'assurer que la segmentation fonctionne
- **Sauvegarder les exports** : Garder les fichiers JSONL pour re-import si besoin

---

**Date de cr√©ation** : 19 D√©cembre 2025  
**Statut** : Pr√™t pour ingestion massive

